{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code should be run in a notebook in Azure Databricks. A prerequisite and manual step: upload files (payments.csv, riders.csv, stations.csv and trips.csv) to Delta using Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv') \\\n",
    "    .option('inferSchema', 'false') \\\n",
    "    .option('header', 'false') \\\n",
    "    .option('sep', ',') \\\n",
    "    .load('/FileStore/tables/demo/payments.csv')\n",
    "df = df.toDF('payment_id', 'date', 'amount', 'rider_id')\n",
    "df.write.format('delta').mode('overwrite').save('/delta/payments')\n",
    "\n",
    "df = spark.read.format('csv') \\\n",
    "    .option('inferSchema', 'false') \\\n",
    "    .option('header', 'false') \\\n",
    "    .option('sep', ',') \\\n",
    "    .load('/FileStore/tables/demo/riders.csv')\n",
    "df = df.toDF('rider_id', 'first', 'last', 'address', 'birthday', 'account_start_date', 'account_end_date', 'is_member')\n",
    "df.write.format('delta').mode('overwrite').save('/delta/riders')\n",
    "\n",
    "df = spark.read.format('csv') \\\n",
    "    .option('inferSchema', 'false') \\\n",
    "    .option('header', 'false') \\\n",
    "    .option('sep', ',') \\\n",
    "    .load('/FileStore/tables/demo/stations.csv')\n",
    "df = df.toDF('station_id', 'name', 'latitude', 'longitude')\n",
    "df.write.format('delta').mode('overwrite').save('/delta/stations')\n",
    "\n",
    "df = spark.read.format('csv') \\\n",
    "    .option('inferSchema', 'false') \\\n",
    "    .option('header', 'false') \\\n",
    "    .option('sep', ',') \\\n",
    "    .load('/FileStore/tables/demo/trips.csv')\n",
    "df = df.toDF('trip_id', 'rideable_type', 'start_at', 'ended_at', 'start_station_id', 'end_station_id', 'rider_id')\n",
    "df.write.format('delta').mode('overwrite').save('/delta/trips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "433ed655-e8cf-4f2b-bbfe-0a6539479085",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql('DROP TABLE IF EXISTS payments')\n",
    "spark.sql('DROP TABLE IF EXISTS riders')\n",
    "spark.sql('DROP TABLE IF EXISTS stations')\n",
    "spark.sql('DROP TABLE IF EXISTS trips')\n",
    "\n",
    "spark.sql(\"CREATE TABLE payments USING DELTA LOCATION '/delta/payments'\")\n",
    "spark.sql(\"CREATE TABLE riders USING DELTA LOCATION '/delta/riders'\")\n",
    "spark.sql(\"CREATE TABLE stations USING DELTA LOCATION '/delta/stations'\")\n",
    "spark.sql(\"CREATE TABLE trips USING DELTA LOCATION '/delta/trips'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d23e7cc0-96a4-47d3-b8e5-e1ef7710f41c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bcce509-4a3f-4c0a-b518-d7b0eb876d1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table('payments')\n",
    "df.write.format('delta').mode('overwrite').saveAsTable('fact_payment')\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83dd33ff-a5f9-48ee-b50c-a833f8b62df7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_1 = spark.table('trips')\n",
    "df_1 = df_1.withColumn(\n",
    "    'time_of_day',\n",
    "    F.when((F.hour(\"start_at\") >= 6) & (F.hour(\"start_at\") <= 11), \"Morning\")\n",
    "     .when((F.hour(\"start_at\") >= 12) & (F.hour(\"start_at\") <= 17), \"Afternoon\")\n",
    "     .when((F.hour(\"start_at\") >= 18) & (F.hour(\"start_at\") <= 21), \"Evening\")\n",
    "     .otherwise(\"Night\")\n",
    ")\n",
    "df_1 = df_1.withColumn(\n",
    "    'trip_duration',\n",
    "    (unix_timestamp(col('ended_at')) - unix_timestamp(col('start_at'))) / 60\n",
    ")\n",
    "\n",
    "df_2 = spark.table('riders').select('rider_id', 'birthday')\n",
    "df_1 = df_1.join(df_2, on = 'rider_id', how = 'inner')\n",
    "df_1 = df_1.withColumn('birthday_timestamp', to_timestamp(col('birthday'), 'yyyy-MM-dd'))\n",
    "df_1 = df_1.withColumn(\n",
    "    'rider_age',\n",
    "    floor((unix_timestamp(col('start_at')) - unix_timestamp(col('birthday_timestamp'))) / (365.25 * 24 * 60 * 60))\n",
    ")\n",
    "df_1 = df_1.drop('birthday_timestamp')\n",
    "\n",
    "order = ['trip_id', 'rideable_type', 'start_at', 'ended_at', 'trip_duration', 'start_station_id', 'end_station_id', 'rider_id', 'rider_age', 'time_of_day']\n",
    "df_1 = df_1.select(*order)\n",
    "\n",
    "df_1.write.format('delta').mode('overwrite').saveAsTable('fact_trip')\n",
    "\n",
    "display(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f34c2f03-0a24-4f82-b280-e8f9c72661bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table('stations')\n",
    "df.write.format('delta').mode('overwrite').saveAsTable('dim_station')\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0ac6528-6ad2-4f4c-8b68-289bc78903f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table('riders')\n",
    "df.write.format('delta').mode('overwrite').saveAsTable('dim_rider')\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "568fd675-150d-4dcd-b12b-d25b05493846",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_1 = spark.table('trips')\n",
    "df_2 = spark.table('payments')\n",
    "\n",
    "df_3 = df_1.select(F.col('start_at').alias('date')) \\\n",
    "    .union(df_2.select(F.col('date').alias('date'))) \\\n",
    "    .distinct()\n",
    "\n",
    "df_3 = df_3.withColumn('day_of_week', F.date_format('date', 'EEEE')) \\\n",
    "           .withColumn('month', F.month('date')) \\\n",
    "           .withColumn('quarter', F.quarter('date')) \\\n",
    "           .withColumn('year', F.year('date'))\n",
    "\n",
    "spark.sql('DROP TABLE IF EXISTS dim_date')\n",
    "df_3.write.format('delta').mode('overwrite').saveAsTable('dim_date')\n",
    "\n",
    "display(df_3)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
